---
# blog page must be named index.html and in its own folder to support pagination
# https://jekyllrb.com/docs/pagination/
layout: page
title: Proj
---
<!--<section class="list">
	{% if site.posts.size == 0 %}
		<p class="text-center">Nothing published yet!</p>
	{% elsif site.paginate %}
		{% for post in paginator.posts %}
	        {% include blog-post.html %}
	    {% endfor %}

		{% if site.paginate %}
			{% include pagination.html%}
		{% endif %}
	{% else %}
		{% for post in site.posts %}
			{% include blog-post.html %}
		{% endfor %}
	{% endif %}
</section>-->

<center>
<h2> Minimalist Grammars Parsing as a Model of Human Sentence Processing</h2>
</center>

<p  style="text-align:justify">
Computationally specified parsing algorithms can be used to ask questions
 about human processing behavior by connecting linguistics, psychology, and computer science.
My most recent work in this direction uses  Stabler (2013)’s top-down parser for Minimalist grammars (MGs)
 to predict processing difficulty based on how the structures built by the parser affects memory usage.
 </p>
 
  
 <p  style="text-align:justify">
For instance, I have shown how the MG parser correctly predicts
  preverbal vs. postverbal subject preferences in Italian, across a variety of constructions.
 </p> 
 
 
 <ul>
<li>
A. De Santo. <A HREF="https://aniellodesanto.github.io/publications/CMCL19_CR.pdf">Testing a Minimalist Grammars Parser on Italian Relative Clause Asymmetries</A>. Proceedings of Cognitive Modeling and Computational
Linguistics Workshop (CMCL) 2019.
</ul>
 
 
  <p  style="text-align:justify">
  In my dissertation, I expand on this work by defining complexity metrics based on cognitively plausible assumptions about human memory mechanisms.
   Through these metrics, I am also exploring the contribution of grammatical features to memory
consumption, evaluating the model’s performance by looking at a variety of syntactic priming
effects reported by psycholinguists. By investigating the unique contribution of the feature-
driven grammar to sentence processing, this approach will clarify the link between grammatical
knowledge and processing behavior.
   </p>


<p  style="text-align:justify">
I am also interested in using this model to bear on more general
theoretical debates in the syntactic literature, thanks to its high sensitivity to fine-grained syntactic structure.
As an example, I have been exploring the MG parser as a formal model of how gradient acceptability can arise from categorical grammars
 </p>
 
  <ul>
<li>
A. De Santo. <A HREF="https://aniellodesanto.github.io/publications/DeSanto_SCiL20.pdf">MG Parsing as a Model of Gradient Acceptability in Syntactic Islands</A>. Proceedings of the Society for Computation in Linguistics (SCiL) 2020.
</ul>

<p  style="text-align:justify">
Moreover,  Nazila Shafiei and I have also been arguing for using parsing models with a computationally explicit linking hypothesis
to have experimental results guide our choice of syntactic analyses.
As a case study, we have looked at alternatives in the structure of Persian relative clauses.
 </p>
 
 <ul>
<li>  De Santo, A and Shafiei, N. On the Structure of Relative Clauses in Persian: Evidence from Computational Modeling and Processing Effects. Talk at the 2nd North American Conference in Iranian Linguistics (NACIL2), April 19-21 2019, University of Arizona.
   </li>
</ul>


<center>
<h2> The Subregular Complexity of Linguistic Dependencies</h2>
</center>

<p  style="text-align:justify">
In the past few years, I have been particularly interested in the study of linguistic patterns from a formal language-theoretical perspective, particularly in the framework of the subregular hierarchy.
<A HREF="https://www.facebook.com/robertadal/videos/vb.36919350/10102918907039060/?type=2&video_source=user_video_tab">Here</A>,
 you can watch me talk about how subregular characterizations highlight core parallels between phonology and syntax (thanks to <A HREF="https://www.robertadalessandro.it/">Roberta D'Alessandro</A> for the video!).
My work in this area can be divided in several sub-projects.
</p>


<p  style="text-align:justify">
From the formal side, I've proposed typologically grounded extensions to the class of tier-based strictly local dependencies.
</p>

<ul>
<li>
De Santo, A. (2018b). Extending TSL to account for interactions of local and non-local constraints. Poster presented at the Society for Computation in Linguistics (SCiL) 2018, Salt Lake City, Utah.
</li>
<li>
De Santo, A. and Graf, T. (2018). Structure sensitive tier projection: Applications and formal properties. In prep. Stony Brook University. (email me for a draft)
</li>
</ul>

<p  style="text-align:justify">
Together with <A HREF="http://artsites.uottawa.ca/kevin-mcmullin/en/research/">Kevin McMullin</A> and <A HREF="https://www.aaksenova.com/">Al&euml;na Aks&euml;nova</A>, I'm now working on efficient learning algorithms for these classes.
</p>


<ul>
<li>
McMullin, K., Aks&euml;nova, A., and De Santo, A. (2019). <a href="https://scholarworks.umass.edu/scil/vol2/iss1/55/">Learning phonotactic restrictions on multiple tiers.</a>Talk  at SCiL 2019, New York City, NY.
</li>
</ul>

<p  style="text-align:justify">
Moreover, I believe that our formal understanding of these classes can help us design better artificial grammar learning experiments, and target precise biases in human learning.
</p>

<ul>
<li>
De Santo, A. (2018). <a href="https://10.3389/fpsyg.2018.00276">Commentary: Developmental constraints on learning artificial grammars with fixed, flexible, and free word order.</a> Frontiers in Psychology, 9:276.
</li>
</ul>

<p  style="text-align:justify">
Formal language theory can also help us settle long-standing linguistics debates.
For example, <A HREF="https://www.aaksenova.com/">Al&euml;na</A> and I have used this approach to argue in favor of derivational representations in morphology.
</p>

<ul>
<li>
Aks&euml;nova, A. and De Santo, A. (2018). <a href="CLS53_Aksenova_DeSanto.pdf">Strict locality in morphological derivations.</a> In Proceedings of CLS 53.
</li>
</ul>



<center>
<h2>Generalized Quantifiers: Memory, Verification, and Priming</h2>
</center>

<p  style="text-align:justify">
In the study of generalized quantifiers, it is essential to have an insightful theory of how their meaning is computed.
In particular, I've been interested in exploring how different quantifiers (aristotelian, cardinal, proportional)
engage memory resources during encoding and verification, and how these effects relate to theories based on the approximate number system or more precise counting systems (e.g. the semantic automata model).
In this line of inquiry, I have relied on experimental techniques such as pupillometry, and EEG.
</p>

 <ul>
<li>
A. De Santo and J. E. Drury. Encoding and Verification Effects of Generalized Quantifiers on Working Memory. Proceedings of CLS54.
(email me for a draft).
  </li>
</ul>

<p  style="text-align:justify">
In a related project Jon Rawski, John Drury, Amanda Yazdani, and I have begun exploring how quantified sentences can be used 
to pinpoint specific ERP markers of strategy switching during truth-value verification 
 and to understand how linguistic meaning and visual context interact during language processing.
</p>

 <ul>
<li>
De Santo, A., Rawski, J., Yazdani, A., and Drury, J. E. (2018). <a href="CLS54_paper.pdf">Quantified sentences as a window into prediction and priming: An ERP study.</a> Proceedings of CLS54.
  </li>
</ul>